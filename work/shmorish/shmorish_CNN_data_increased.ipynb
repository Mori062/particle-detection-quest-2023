{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3095fde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 評価方法\n",
    "\n",
    "分類されていないデータを認識し、どれだけ正しくカテゴリごとに分類できるかを算出した「平均精度」の高さを競い合います。\n",
    "\n",
    "今回、活用するデータはLSWMD_25519となります。\n",
    "LSWMD_25519のFailureType項目が分類されていない状態のデータに対し、正しいFailureTypeカテゴリを分類するプログラムを作成し、その平均精度を算出します。\n",
    "平均精度とは、カテゴリごとに正しく分類できる精度を平均した値です。カテゴリごとに算出した精度（Aが正しく分類された数/Aのデータ数）を足し、カテゴリ数で割ります。\n",
    "\n",
    "公平な評価を実施するために、以下の制限を設けています。\n",
    "1. 外部パッケージをインストールするためのセルとsolution関数の中身のみを編集すること\n",
    "2. 校舎のiMac上で最後のセルの実行時間が15分未満であること　（%%timeitの出力結果を確認してください）\n",
    "\n",
    "※気になる点がある場合、Discordで気軽にお問合せください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0f68e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/Users/morishitashoto/.pyenv/versions/3.10.0/bin/python' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/morishitashoto/.pyenv/versions/3.10.0/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np # https://numpy.org/ja/\n",
    "import pandas as pd # https://pandas.pydata.org/\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea86471-32fa-46c6-a005-ad6e1f7b7f72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "外部パッケージを使用する場合、以下の方法でインストールを実施してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be8d1cd-7df7-4b10-aa1a-e24677b50d78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 必要な外部パッケージは、以下の内容を編集しインストールしてください\n",
    "# !pip install keras\n",
    "# !pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113ea05-433a-4c82-9cb1-2c8f834a0364",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "以下のsolution関数のみ編集してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda6ad7e-8e26-4208-bdbd-3fa37e79c82c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# def get_x_data(df, is_fit=True, size=1.0):\n",
    "#     target_size = (28, 28)\n",
    "#     resized_images = []\n",
    "#     for img in df['waferMap']:\n",
    "#         resized_img = resize(img, target_size, anti_aliasing=True)\n",
    "#         resized_images.append(resized_img)\n",
    "#     _X_data = np.array(resized_images)\n",
    "#     X_data = torch.tensor(_X_data, dtype=torch.float)\n",
    "#     X_data = X_data.unsqueeze(1)\n",
    "#     return X_data\n",
    "\n",
    "def get_x_data(df, is_fit=True, size=1.0):\n",
    "    target_size = (28, 28)\n",
    "    resized_images = []\n",
    "    for img in df['waferMap']:\n",
    "        resized_img = resize(img, target_size, anti_aliasing=True)\n",
    "        resized_images.append(resized_img)\n",
    "        rotated_img_90 = np.rot90(resized_img, k=1, axes=(0, 1))\n",
    "        rotated_img_180 = np.rot90(resized_img, k=2, axes=(0, 1))\n",
    "        rotated_img_270 = np.rot90(resized_img, k=3, axes=(0, 1))\n",
    "\n",
    "        resized_images.extend([rotated_img_90, rotated_img_180, rotated_img_270])\n",
    "\n",
    "    _X_data = np.array(resized_images)\n",
    "    X_data = torch.tensor(_X_data, dtype=torch.float)\n",
    "    X_data = X_data.unsqueeze(1)\n",
    "    return X_data\n",
    "\n",
    "def solution(x_test_df, train_df):\n",
    "    le = LabelEncoder()\n",
    "    X_train = get_x_data(train_df)\n",
    "    y_train = le.fit_transform(np.array(train_df['failureType']))\n",
    "    X_test = get_x_data(x_test_df, False)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Normalize data\n",
    "    normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    # Create DataLoader with data augmentation\n",
    "    train_dataset = TensorDataset(X_train, torch.tensor(y_train, dtype=torch.long).repeat(4)[:len(X_train)])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = Net()\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Adjust the learning rate\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    epoch = 30  # Increase the number of epochs\n",
    "    for i in range(1, epoch + 1):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            rotated_inputs = torch.rot90(inputs, k=1, dims=(2, 3))\n",
    "            flipped_inputs = torch.flip(inputs, dims=(3,))\n",
    "            rotated_inputs_180 = torch.rot90(inputs, k=2, dims=(2, 3))\n",
    "            rotated_inputs_270 = torch.rot90(inputs, k=3, dims=(2, 3))\n",
    "\n",
    "            augmented_inputs = torch.cat([inputs, rotated_inputs, flipped_inputs, rotated_inputs_180, rotated_inputs_270], dim=0)\n",
    "            augmented_labels = torch.cat([labels] * 5, dim=0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(augmented_inputs)\n",
    "            loss = criterion(outputs, augmented_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {i}/{epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        # if loss.item() < best_loss:\n",
    "        #     best_loss = loss.item()\n",
    "        #     counter = 0\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     if counter >= patience:\n",
    "        #         print(f\"Early stopping at epoch {i} due to no improvement in {patience} epochs.\")\n",
    "        #         break\n",
    "\n",
    "    # Move model back to CPU before predicting\n",
    "    model.to(\"cpu\")\n",
    "    outputs = model(torch.tensor(X_test, dtype=torch.float))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_pred = le.inverse_transform(predicted.numpy())\n",
    "\n",
    "    return pd.DataFrame({'failureType': y_pred}, index=x_test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c20f4-f775-4d9d-90c7-a3b583584edd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "solution関数は以下のように活用され、平均精度を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a31dda-7c8b-477e-9547-5c9db739f7f0",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均精度：11.94%\n",
      "106 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# データのインポート\n",
    "df=pd.read_pickle(\"../input/LSWMD_25519.pkl\")\n",
    "\n",
    "# テスト用と学習用のデータを作成（テストする際は、random_stateの値などを編集してみてください）\n",
    "train_df, test_df = train_test_split(df, stratify=df['failureType'], test_size=0.10, random_state=42)\n",
    "\n",
    "y_test_df = test_df[['failureType']]\n",
    "x_test_df = test_df.drop(columns=['failureType'])\n",
    "\n",
    "# solution関数を実行\n",
    "user_result_df = solution(x_test_df, train_df)\n",
    "\n",
    "average_accuracy = 0\n",
    "# ユーザーの提出物のフォーマット確認\n",
    "if type(y_test_df) == type(user_result_df) and y_test_df.shape == user_result_df.shape:\n",
    "    # 平均精度の計算\n",
    "    accuracies = {}\n",
    "    for failure_type in df['failureType'].unique():\n",
    "        y_test_df_by_failure_type = y_test_df[y_test_df['failureType'] == failure_type]\n",
    "        user_result_df_by_failure_type = user_result_df[y_test_df['failureType'] == failure_type]\n",
    "        matching_rows = (y_test_df_by_failure_type == user_result_df_by_failure_type).all(axis=1).sum()\n",
    "        accuracies[failure_type] = (matching_rows/(len(y_test_df_by_failure_type)))\n",
    "    \n",
    "    average_accuracy = sum(accuracies.values())/len(accuracies)\n",
    "\n",
    "print(f\"平均精度：{average_accuracy*100:.2f}%\")\n",
    "t2 = time.time()\n",
    "print(f\"実行時間：{t2-t1:.2f}秒\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = df['failureType'].unique()\n",
    "cm = confusion_matrix(y_pred=user_result_df['failureType'], y_true=y_test_df['failureType'], labels=labels)\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "cmp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
